# -*- coding: utf-8 -*-
"""Comparison_perplexity.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13ztsY-SO1ZAqkqcL7raQ5zDeUQDOvdtJ
"""

# This notebook evaluates different sentence language models using Perplexity metric
# on sentence correctness

# Transformers installation
! pip install transformers datasets sentencepiece

# To install from source instead of the last release, comment the command above and uncomment the following one.
# ! pip install git+https://github.com/huggingface/transformers.git

# Sample Sentences
incorrect = [
    "Our current population is 6 billion people and it is still growing exponentially.",
    "This will, if not already, caused  problems as there are very limited spaces for us.",
    "A manager should always be honest with their employees.",
    "They cooked the dinner themself.",
    "If I will be in London, I will contact to you."
]
correct = [
    "Our current population is 6 billion people, and it is still growing exponentially.",
    "This will, if not already, cause problems as there are very limited spaces for us.",
    "A manager should always be honest with his employees.",
    "They cooked the dinner themselves.",
    "If I am in London, I will contact you."
        
]

"""# BART"""

from transformers import BartTokenizerFast, BartForConditionalGeneration
from transformers import AutoTokenizer
from transformers import AutoModelForMaskedLM
import torch
import numpy as np

with torch.no_grad():
  model_checkpoint = 'bart'
  model_name ='facebook/bart-base'
  # model = AutoModelForMaskedLM.from_pretrained(model_name)
  model = BartForConditionalGeneration.from_pretrained(model_name, return_dict=True)
  model.eval()

# Load pre-trained model tokenizer (vocabulary)
tokenizer = BartTokenizerFast.from_pretrained(model_name)
# tokenizer  = AutoTokenizer.from_pretrained(model_name) # bart-large is the sam

def score(sentence):
    tokenize_input = tokenizer.encode(sentence)
    tensor_input = torch.tensor([tokenize_input])
    loss=model(tensor_input, labels=tensor_input)[0]
    return np.exp(loss.detach().numpy())
 
print(f"Incorrect sentence: ")
print([score(i) for i in incorrect])
print(f"Correct sentence: ")
print([score(j) for j in correct])

"""# T5"""

from transformers import T5Tokenizer, T5ForConditionalGeneration
import torch

with torch.no_grad():
  model = T5ForConditionalGeneration.from_pretrained("t5-base")
  model.eval()

  # Load pre-trained model tokenizer (vocabulary)
tokenizer = T5Tokenizer.from_pretrained("t5-base")

def score(sentence):
    tokenize_input = tokenizer.encode(sentence)
    tensor_input = torch.tensor([tokenize_input])
    loss=model(tensor_input, labels=tensor_input)[0]
    return np.exp(loss.detach().numpy())
 
print(f"Incorrect sentence: ")
print([score(i) for i in incorrect])
print(f"Correct sentence: ")
print([score(j) for j in correct])