# -*- coding: utf-8 -*-
"""train_grammar_correction_t5.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mWEqjpepUBslCnVazQ_bJHDOCISyIyfn

# This notebook is used for training a transformer language model (t5) for the **Grammar Correction Task**

## Installation
"""

pip install happytransformer datasets

"""## Model """

from happytransformer import HappyTextToText

happy_tt = HappyTextToText("T5", "t5-base")

"""## Data Collection  """

from datasets import load_dataset

train_dataset = load_dataset("jfleg", split='validation[:]')

eval_dataset = load_dataset("jfleg", split='test[:]')

"""## Data Examination  """

for case in train_dataset["corrections"][:2]:
  print(case)
  print(case[0])
  print("--------------------------------------------------------")

"""## Data Preprocessing   """

import csv

def generate_csv(csv_path, dataset):
    with open(csv_path, 'w', newline='') as csvfile:
        writter = csv.writer(csvfile)
        writter.writerow(["input", "target"])
        for case in dataset:
            input_text = "grammar: " + case["sentence"]
            for correction in case["corrections"]:
                # a few of the cases are blank strings. So we'll skip them
                if input_text and correction:
                    writter.writerow([input_text, correction])

generate_csv("train.csv", train_dataset)
generate_csv("eval.csv", eval_dataset)

"""## Before Training Evaluating"""

before_result = happy_tt.eval("eval.csv")

print("Before loss: ", before_result.loss)

"""## Train"""

from happytransformer import TTTrainArgs

args = TTTrainArgs(batch_size=8)

happy_tt.train("train.csv", args=args)

"""## After Training Evaluating"""

after_result = happy_tt.eval("eval.csv")

print("After loss:", after_result.loss)

"""## Inference"""

from happytransformer import TTSettings

beam_settings =  TTSettings(num_beams=5, min_length=1, max_length=20)

"""### Example 1"""

example_1 = "This sentences, has bads grammar and spelling!"

result_1 = happy_tt.generate_text(example_1, args=beam_settings)
print(result_1.text)

"""### Example 2"""

example_2 = "I am enjoys, writtings articles ons AI."

result_2 = happy_tt.generate_text(example_2, args=beam_settings)
print(result_2.text)

"""### Example 3"""

example_3 = "The solution can be obtain by using technology to achieve a better usage of space that we have and resolve the problems in lands that  inhospitable  such as desserts  and swamps."

result_3 = happy_tt.generate_text(example_3, args=beam_settings)
print(result_3.text)

"""## Data Preprocessing """

replacements = [
  (" .", "."), 
  (" ,", ","),
  (" '", "'"),
  (" ?", "?"),
  (" !", "!"),
  (" :", "!"),
  (" ;", "!"),
  (" n't", "n't"),
  (" v", "n't"),
  ("2 0 0 6", "2006"),
  ("5 5", "55"),
  ("4 0 0", "400"),
  ("1 7-5 0", "1750"),
  ("2 0 %", "20%"),
  ("5 0", "50"),
  ("1 2", "12"),
  ("1 0", "10"),
  ('" ballast water', '"ballast water')

]

def remove_excess_spaces(text):
  for rep in replacements:
    text = text.replace(rep[0], rep[1])

  return text

remove_excess_spaces("Hi , your seat number is 5 5 .")

def generate_csv_updated(csv_path, dataset):
    with open(csv_path, 'w', newline='') as csvfile:
        writter = csv.writer(csvfile)
        writter.writerow(["input", "target"])
        for case in dataset:
            input_text = "grammar: " + case["sentence"]
            for correction in case["corrections"]:
                # a few of the cases are blank strings. So we'll skip them
                if input_text and correction:
                  input_text = remove_excess_spaces(input_text)
                  correction = remove_excess_spaces(correction)
                  writter.writerow([input_text, correction])

generate_csv_updated("train_2.csv", train_dataset)

"""## Pretrained Model"""

happy_tt = HappyTextToText("T5", "vennify/t5-base-grammar-correction")

result = happy_tt.generate_text("grammar: I boughts ten apple.", args=beam_settings)
print(result.text)